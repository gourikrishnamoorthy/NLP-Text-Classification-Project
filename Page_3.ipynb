{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "import time\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of all the parameters that will try on the data - from this week\n",
    "all_params = {1:{ \"Pipeline\" : [ ('vectorizer', CountVectorizer()),\n",
    "                    ('classifier', DecisionTreeClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0],\n",
    "                       'classifier__max_depth': [10, 20, 30],\n",
    "                       'classifier__min_samples_split': [2, 5, 10],\n",
    "                       'classifier__min_samples_leaf':[3,5]\n",
    "                    },\n",
    " },\n",
    " 2:{ \"Pipeline\" : [ ('vectorizer', TfidfVectorizer()),\n",
    "                    ('classifier', DecisionTreeClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0],\n",
    "                       'classifier__max_depth': [10, 20, 30],\n",
    "                       'classifier__min_samples_split': [2, 5, 10],\n",
    "                       'classifier__min_samples_leaf':[3,5]\n",
    "                    },\n",
    " },\n",
    " 3:{ \"Pipeline\" : [ ('vectorizer', CountVectorizer()),\n",
    "                    ('classifier', BaggingClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0],\n",
    "                       'classifier__n_estimators': [10, 50, 100]\n",
    "                    },\n",
    " },\n",
    " 4:{ \"Pipeline\" : [ ('vectorizer', TfidfVectorizer()),\n",
    "                    ('classifier', BaggingClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0],\n",
    "                       'classifier__n_estimators': [10, 50, 100]\n",
    "                    },\n",
    " },\n",
    " 5:{ \"Pipeline\" : [ ('vectorizer', CountVectorizer()),\n",
    "                    ('classifier', RandomForestClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0]\n",
    "                    },\n",
    " },\n",
    " 6:{ \"Pipeline\" : [ ('vectorizer', TfidfVectorizer()),\n",
    "                    ('classifier', RandomForestClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0]\n",
    "                    },\n",
    " },\n",
    "7:{ \"Pipeline\" : [ ('vectorizer', CountVectorizer()),\n",
    "                    ('classifier', ExtraTreesClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0],\n",
    "                       'classifier__bootstrap':[True, False]\n",
    "                    },\n",
    " },\n",
    " 8:{ \"Pipeline\" : [ ('vectorizer', TfidfVectorizer()),\n",
    "                    ('classifier', ExtraTreesClassifier(random_state=42))],\n",
    "     \"hyper_params\": { 'vectorizer__max_features': [1000, 2000, 5000],\n",
    "                       'vectorizer__ngram_range': [(1,3)],\n",
    "                       'vectorizer__stop_words': ['english'],\n",
    "                       'vectorizer__min_df':[1],\n",
    "                       'vectorizer__max_df':[1.0],\n",
    "                       'classifier__bootstrap':[True, False]\n",
    "                     }\n",
    " }\n",
    "}\n",
    "\n",
    "# all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bold(x):\n",
    "    open_tag_bold = \"\\033[1m\"\n",
    "    close_tag_bold = \"\\033[0m\"\n",
    "    return open_tag_bold + str(x) + close_tag_bold\n",
    "\n",
    "# display scores \n",
    "def display_scores(scores, no_of_posts):\n",
    "    \n",
    "    print(\"\\nMetrics for {} posts\".format(get_bold(no_of_posts)))\n",
    "    df = pd.DataFrame(scores, index=list(range(1,4)))\n",
    "    df = df.sort_values(by=\"Accuracy\", ascending=True)\n",
    "#     display(df)\n",
    "    display(HTML(df.to_html()))\n",
    "#     sns.set(style=\"darkgrid\")\n",
    "    df[\"Feature Set\"] = df.index\n",
    "    nrows = 1 # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=8, figsize=(18,3)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    col_list = [\"Accuracy\", \"Mis Calculations\",\"ROC AUC\",\"Sensitivity\",\"Specificity\",\"Precision\", \"Test Scores\",\"Train Scores\"]\n",
    "    col_colr = [\"blue\", \"slategray\", \"green\", \"sienna\", \"teal\", \"firebrick\", \"lightseagreen\", \"mediumorchid\"]\n",
    "    for i, column in enumerate(col_list): # Gives us an index value to get into all our lists\n",
    "        sns.scatterplot(df[column], df[\"Feature Set\"],marker=\"o\", size=df[column], legend=False, facecolor=col_colr[i], edgecolor=col_colr[i], ax=ax[i])\n",
    "        ax[i].set_ylabel(\"\");\n",
    "\n",
    "#highlight minimum value form the data frame\n",
    "def highlight_min(s):    \n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_min]\n",
    "\n",
    "#highlight max value from the data frame\n",
    "def highlight_max(s):    \n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "#combine full row into a single row\n",
    "def get_full_row(data, features):\n",
    "    data[\"full_row\"] = \"\"\n",
    "    for col in features:\n",
    "        data[\"full_row\"] = data[\"full_row\"] + \",\" + data[col].astype(str)\n",
    "        \n",
    "\n",
    "#return best estimator results\n",
    "def get_score_data(results, X_train, X_test, y_train,y_test):\n",
    "    output = []\n",
    "    sub_scores = {}\n",
    "    \n",
    "    model = results.best_estimator_\n",
    "    \n",
    "    #confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, model.predict(X_test)).ravel()\n",
    "    \n",
    "    ##calulating accuracy\n",
    "#     accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    accuracy = round(accuracy_score(y_test, model.predict(X_test)),2)\n",
    "    \n",
    "    #calculating Misclassification Rate\n",
    "    mis_calcuations = 1 - accuracy\n",
    "    \n",
    "    #calculating sensitivity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "\n",
    "    #calculating specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    #calculating precision\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    #to predict roc_auc_score\n",
    "    pred_proba = [i[1] for i in model.predict_proba(X_test)]\n",
    "\n",
    "    pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})\n",
    "    \n",
    "    #For returning results from the best estimator\n",
    "    #1. best score\n",
    "    output.append(round(results.best_score_,2))\n",
    "    \n",
    "    #2.best params\n",
    "    output.append(results.best_params_)\n",
    "    \n",
    "    #No of 0's and 1's in test\n",
    "    sub_scores.update({\"No of evolution posts\": y_test[y_test == 0].count()})\n",
    "    sub_scores.update({\"No of Creation posts\": y_test[y_test == 1].count()})\n",
    "    \n",
    "    #No of 0's and 1's predicted\n",
    "    df = pd.DataFrame({\"Preds\": model.predict(X_test)})\n",
    "    try:\n",
    "        sub_scores.update({\"No of predicted evolution posts\": df.groupby(\"Preds\")[\"Preds\"].value_counts()[0].values[0]})\n",
    "    except:\n",
    "        sub_scores.update({\"No of predicted evolution posts\":0})\n",
    "    \n",
    "    try:\n",
    "        sub_scores.update({\"No of predicted creations posts\": df.groupby(\"Preds\")[\"Preds\"].value_counts()[1].values[0]})\n",
    "    except:\n",
    "        sub_scores.update({\"No of predicted creations posts\":0})\n",
    "    \n",
    "    #baseline\n",
    "    sub_scores.update({\"Baseline accuracy%\": round(y_test.value_counts(normalize=True)[0],2)})\n",
    "    \n",
    "    #3.Train Score\n",
    "    sub_scores.update({\"Train Scores\": round(model.score(X_train,y_train),2)})\n",
    "    \n",
    "    #4.Test Score\n",
    "    sub_scores.update({\"Test Scores\": round(model.score(X_test,y_test),2)})\n",
    "    \n",
    "    #5.Accuracy\n",
    "    sub_scores.update({\"Accuracy\": accuracy})\n",
    "    \n",
    "    #6.Mis Calculations\n",
    "    sub_scores.update({\"Mis Calculations\": round(mis_calcuations,2)})\n",
    "\n",
    "    #7.Sensitivity\n",
    "    sub_scores.update({\"Sensitivity\": round(sensitivity,2)})\n",
    "    \n",
    "    #8.Specificity\n",
    "    sub_scores.update({\"Specificity\": round(specificity,2)})\n",
    "\n",
    "    #9.Precision\n",
    "    sub_scores.update({\"Precision\": round(precision,2)})\n",
    "\n",
    "    #9.ROC AUC\n",
    "    sub_scores.update({\"ROC AUC\": round(round(roc_auc_score(pred_df['true_values'], pred_df['pred_probs']),2))})\n",
    "    \n",
    "    10.\n",
    "    if model.score(X_train,y_train) > model.score(X_test,y_test):\n",
    "        sub_scores.update({\"Fit Type\":\"Overfit\"})\n",
    "    else:\n",
    "        sub_scores.update({\"Fit Type\":\"Underfit\"})\n",
    "        \n",
    "    output.append(sub_scores)\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#simple model,fit,evaluate, and return a list of best estimators and \n",
    "def model_fit_score(X, y, best_scores, feature_set_no):\n",
    "     \n",
    "        #Step 1 : split the data into test/train\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, shuffle=True, test_size=0.33, stratify=y, random_state = 42)\n",
    "\n",
    "        for param in all_params:\n",
    "\n",
    "            pipe = Pipeline(all_params[param][\"Pipeline\"])\n",
    "            hyper_params = all_params[param][\"hyper_params\"]\n",
    "             \n",
    "            # Perform Grid Search\n",
    "            gridcv = GridSearchCV(pipe, \n",
    "                                  param_grid=hyper_params,\n",
    "                                  cv = 5,\n",
    "                                  scoring = \"accuracy\")\n",
    "            #results\n",
    "            results = gridcv.fit(X_train, y_train)\n",
    "            best_scores[param] = get_score_data(results, X_train, X_test, y_train,y_test)\n",
    "            #wait for 5 seconds until moving on\n",
    "            time.sleep(5)\n",
    "            print(\"Round {} complete for feature set {}\".format(param, feature_set_no))\n",
    "        \n",
    "        return best_scores\n",
    "            \n",
    "    \n",
    "#combine two subreddits\n",
    "def combine(subreddit1, subreddit2):\n",
    "    all_data_df = pd.concat((subreddit1, subreddit2), axis=0)\n",
    "    sub1 = subreddit1[\"subreddit\"].unique()[0]\n",
    "    sub2 = subreddit2[\"subreddit\"].unique()[0]\n",
    "    \n",
    "    all_data_df[\"y\"] = all_data_df[\"subreddit\"].map({sub1: 0, sub2: 1})\n",
    "    \n",
    "    return all_data_df\n",
    "\n",
    "         \n",
    "def lemmatize(s):\n",
    "    list_words = s.split(\",\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ','.join([lemmatizer.lemmatize(word) for word in list_words])\n",
    "\n",
    "def stemmer(s):\n",
    "    list_words = s.split(\",\")\n",
    "    stemmer = PorterStemmer()\n",
    "    return ','.join([stemmer.stem(word) for word in list_words])\n",
    "\n",
    "feature_sets={\"1\": [\"title\"],\n",
    "              \"2\": [\"title\", \"selftext\"],\n",
    "              \"3\": [\"title\", \"selftext\", \"comment\"],\n",
    "             }\n",
    "\n",
    "\n",
    "#EDA for each run\n",
    "def get_best_scores_params(run):\n",
    "    evolution_sub_clean_df = pd.read_csv(\"./datasets/evolution_sub_clean_\" + run+ \".csv\")\n",
    "    creation_sub_clean_df = pd.read_csv(\"./datasets/creation_sub_clean_\" + run + \".csv\")\n",
    "    all_data_df = combine(evolution_sub_clean_df, creation_sub_clean_df)    \n",
    "\n",
    "    \n",
    "    best_scores = {x:{y:[] for y in all_params} for x in feature_sets}\n",
    "\n",
    "    for features in feature_sets:\n",
    "        get_full_row(all_data_df, feature_sets[features])\n",
    "        model_fit_score(all_data_df[\"full_row\"], all_data_df[\"y\"], best_scores[features], features)\n",
    "\n",
    "            \n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 complete for feature set 1\n",
      "Round 2 complete for feature set 1\n",
      "Round 3 complete for feature set 1\n",
      "Round 4 complete for feature set 1\n",
      "Round 5 complete for feature set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 6 complete for feature set 1\n",
      "Round 7 complete for feature set 1\n",
      "Round 8 complete for feature set 1\n",
      "Round 1 complete for feature set 2\n",
      "Round 2 complete for feature set 2\n",
      "Round 3 complete for feature set 2\n",
      "Round 4 complete for feature set 2\n",
      "Round 5 complete for feature set 2\n",
      "Round 6 complete for feature set 2\n",
      "Round 7 complete for feature set 2\n",
      "Round 8 complete for feature set 2\n",
      "Round 1 complete for feature set 3\n",
      "Round 2 complete for feature set 3\n",
      "Round 3 complete for feature set 3\n",
      "Round 4 complete for feature set 3\n",
      "Round 5 complete for feature set 3\n",
      "Round 6 complete for feature set 3\n",
      "Round 7 complete for feature set 3\n",
      "Round 8 complete for feature set 3\n"
     ]
    }
   ],
   "source": [
    "best_scores = get_best_scores_params(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {x:{y:None for y in all_params} for x in feature_sets}\n",
    "miss_calc_dict = {x:{y:None for y in all_params} for x in feature_sets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features in feature_sets:\n",
    "    for params in all_params:\n",
    "        metrics_dict[features][params] = best_scores[features][params][0]\n",
    "for features in feature_sets:\n",
    "    for params in all_params:\n",
    "        miss_calc_dict[features][params] = best_scores[features][params][2]['Mis Calculations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_9aeb67ce_43b9_11ea_a2aa_80e650232554row6_col0 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_9aeb67ce_43b9_11ea_a2aa_80e650232554row6_col2 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_9aeb67ce_43b9_11ea_a2aa_80e650232554row7_col1 {\n",
       "            background-color:  lightgreen;\n",
       "        }</style><table id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >1</th>        <th class=\"col_heading level0 col1\" >2</th>        <th class=\"col_heading level0 col2\" >3</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row0_col0\" class=\"data row0 col0\" >0.65</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row0_col1\" class=\"data row0 col1\" >0.72</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row0_col2\" class=\"data row0 col2\" >0.74</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row1_col0\" class=\"data row1 col0\" >0.64</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row1_col1\" class=\"data row1 col1\" >0.68</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row1_col2\" class=\"data row1 col2\" >0.67</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row2_col0\" class=\"data row2 col0\" >0.63</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row2_col1\" class=\"data row2 col1\" >0.72</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row2_col2\" class=\"data row2 col2\" >0.73</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row3_col0\" class=\"data row3 col0\" >0.64</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row3_col1\" class=\"data row3 col1\" >0.73</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row3_col2\" class=\"data row3 col2\" >0.73</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row4_col0\" class=\"data row4 col0\" >0.6</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row4_col1\" class=\"data row4 col1\" >0.68</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row4_col2\" class=\"data row4 col2\" >0.72</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row5_col0\" class=\"data row5 col0\" >0.6</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row5_col1\" class=\"data row5 col1\" >0.72</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row5_col2\" class=\"data row5 col2\" >0.69</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row6_col0\" class=\"data row6 col0\" >0.66</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row6_col1\" class=\"data row6 col1\" >0.69</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row6_col2\" class=\"data row6 col2\" >0.75</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row7_col0\" class=\"data row7 col0\" >0.63</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row7_col1\" class=\"data row7 col1\" >0.74</td>\n",
       "                        <td id=\"T_9aeb67ce_43b9_11ea_a2aa_80e650232554row7_col2\" class=\"data row7 col2\" >0.72</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a22175ed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics_dict).style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75,\n",
       " {'classifier__bootstrap': False,\n",
       "  'vectorizer__max_df': 1.0,\n",
       "  'vectorizer__max_features': 2000,\n",
       "  'vectorizer__min_df': 1,\n",
       "  'vectorizer__ngram_range': (1, 3),\n",
       "  'vectorizer__stop_words': 'english'},\n",
       " {'No of evolution posts': 33,\n",
       "  'No of Creation posts': 33,\n",
       "  'No of predicted evolution posts': 41,\n",
       "  'No of predicted creations posts': 25,\n",
       "  'Baseline accuracy%': 0.5,\n",
       "  'Train Scores': 0.99,\n",
       "  'Test Scores': 0.76,\n",
       "  'Accuracy': 0.76,\n",
       "  'Mis Calculations': 0.24,\n",
       "  'Sensitivity': 0.64,\n",
       "  'Specificity': 0.88,\n",
       "  'Precision': 0.84,\n",
       "  'ROC AUC': 1.0,\n",
       "  'Fit Type': 'Overfit'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##best score from the above board, looking at the Scores in details\n",
    "best_scores[\"3\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row1_col2 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row2_col2 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row4_col1 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row4_col2 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row6_col0 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row6_col1 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row6_col2 {\n",
       "            background-color:  lightgreen;\n",
       "        }    #T_b5b68dd6_43b9_11ea_a2aa_80e650232554row7_col0 {\n",
       "            background-color:  lightgreen;\n",
       "        }</style><table id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >1</th>        <th class=\"col_heading level0 col1\" >2</th>        <th class=\"col_heading level0 col2\" >3</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row0_col0\" class=\"data row0 col0\" >0.48</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row0_col1\" class=\"data row0 col1\" >0.26</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row0_col2\" class=\"data row0 col2\" >0.32</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row1_col0\" class=\"data row1 col0\" >0.5</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row1_col1\" class=\"data row1 col1\" >0.27</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row1_col2\" class=\"data row1 col2\" >0.24</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row2_col0\" class=\"data row2 col0\" >0.45</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row2_col1\" class=\"data row2 col1\" >0.26</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row2_col2\" class=\"data row2 col2\" >0.24</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row3_col0\" class=\"data row3 col0\" >0.42</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row3_col1\" class=\"data row3 col1\" >0.24</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row3_col2\" class=\"data row3 col2\" >0.27</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row4_col0\" class=\"data row4 col0\" >0.48</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row4_col1\" class=\"data row4 col1\" >0.21</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row4_col2\" class=\"data row4 col2\" >0.24</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row5_col0\" class=\"data row5 col0\" >0.5</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row5_col1\" class=\"data row5 col1\" >0.3</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row5_col2\" class=\"data row5 col2\" >0.32</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row6_col0\" class=\"data row6 col0\" >0.39</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row6_col1\" class=\"data row6 col1\" >0.21</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row6_col2\" class=\"data row6 col2\" >0.24</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row7_col0\" class=\"data row7 col0\" >0.39</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row7_col1\" class=\"data row7 col1\" >0.23</td>\n",
       "                        <td id=\"T_b5b68dd6_43b9_11ea_a2aa_80e650232554row7_col2\" class=\"data row7 col2\" >0.27</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a22175110>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Highlighting the minimum miss calculations or the minimum difference between test and train scores\n",
    "pd.DataFrame(miss_calc_dict).style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : From the above we can see that, Logistic Regression with tokenizer worked the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
